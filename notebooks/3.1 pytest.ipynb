{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytest: Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This notebook does not work in WebAssembly (no shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small local extension\n",
    "%load_ext save_and_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly recommend taking some time to learn advanced pytest, as anything that makes writing tests easiser enables more and better testing, which is always a plus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests should be easy\n",
    "\n",
    "Always use pytest. The built-in unittest is _very_ verbose; the simpler the writing of tests, the more tests you will write!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_and_run unittest\n",
    "import unittest\n",
    "\n",
    "class MyTestCase(unittest.TestCase):\n",
    "    def test_something(self):\n",
    "        x = 1\n",
    "        self.assertEqual(x, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast this to pytest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_and_run pytest\n",
    "def test_something():\n",
    "    x = 1\n",
    "    assert x == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest still gives you clear breakdowns, including what the value of `x` actually is, even though it seems to use the Python `assert` statement! You don't need to set up a class (though you can), and you don't need to remember 50 or so different `self.assert*` functions! pytest can also run unittest tests, as well as the old `nose` package's tests, too.\n",
    "\n",
    "Approximately equals is normally ugly to check, but pytest makes it easy too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_and_run pytest\n",
    "from pytest import approx\n",
    "\n",
    "def test_approx():\n",
    "    .3333333333333 == approx(1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests should test for failures too\n",
    "\n",
    "\n",
    "You should make sure that expected errors are thrown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_and_run pytest --no-header\n",
    "import pytest\n",
    "\n",
    "def test_raises():\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        1 / 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check for warnings as well, with `pytest.warns` or `pytest.deprecated_call`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests should stay easy when scaling out\n",
    "\n",
    "pytest [uses fixtures](https://docs.pytest.org/en/stable/fixture.html) to represent complex ideas, like setup/teardown, temporary resources, or parameterization.\n",
    "\n",
    "\n",
    "A fixture looks like a function argument; pytest recognizes them by name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_and_run pytest --no-header\n",
    "def test_printout(capsys):\n",
    "    print(\"hello\")\n",
    "    \n",
    "    captured = capsys.readouterr()\n",
    "    assert \"hello\" in captured.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a new fixture is not too hard, and can be placed in the test file or in `conftest.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_and_run pytest --no-header\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[1,2,3], ids=[\"one\", \"two\", \"three\"])\n",
    "def ints(request):\n",
    "    return request.param\n",
    "\n",
    "def test_on_ints(ints):\n",
    "    assert ints**2 == ints*ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have left off `ids`, but for complex inputs, this lets the tests have beautiful names.\n",
    "\n",
    "\n",
    "Now you will get three tests, `test_on_ints-one`, `test_on_ints-two`, and `test_on_ints-three`!\n",
    "\n",
    "Fixtures can be scoped, allowing simple setup/teardown (use `yield` if you need to run teardown). You can even set `autouse=True` to use a fixture always in a file or module (via `conftest.py`). You can have `conftest.py`'s in nested folders, too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an advanced example, which also uses `monkeypatch`, which is a great way for making things hard to split into units into unit tests. Let's say you wanted to make a test that \"tricked\" your code into thinking that it was running on different platforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_and_run pytest --no-header\n",
    "\n",
    "import platform\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=['Linux', 'Darwin', 'Windows'], autouse=True)\n",
    "def platform_system(request, monkeypatch):\n",
    "    monkeypatch.setattr(platform, \"system\", lambda : request.param)\n",
    "    \n",
    "def test_anything():\n",
    "    assert platform.system() in {\"Linux\", \"Darwin\", \"Windows\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every test in the file this is in (or the directory that this is in if in conftest) will run three times, and each time will identify as a different `platform.system()`! Leave `autouse` off, and it becomes opt-in; adding `platform_system` to the list of arguments will opt in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests should be organized\n",
    "\n",
    "You can use `pytest.mark.*` to mark tests, so you can easily turn on or off groups of tests, or do something else special with marked tests, like tests marked \"slow\", for example. Probably the most useful built-in mark is `skipif`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_and_run pytest --no-header\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.skipif(\"sys.version_info < (3, 8)\")\n",
    "def test_only_on_37plus():\n",
    "    x = 3\n",
    "    assert f\"{x = }\" == \"x = 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this test will only run on Python 3.8, and will be skipped on earlier versions. You don't have to use a string for the condition, but if you don't, add a `reason=` so there will still be nice printout explaining why the test was skipped.\n",
    "\n",
    "You can also use `xfail` for tests that are expected to fail (you can even strictly test them as failing if you want). You can use `parametrize` to make a single parameterized test instead of sharing them (with fixtures). There's a `filterwarnings` mark, too.\n",
    "\n",
    "Many pytest plugins support new marks too, like `pytest-parametrize`. You can also use custom marks to enable/disable groups of tests, or to pass data into fixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests should test the installed version, not the local version\n",
    "\n",
    "Your tests should run against an _installed_ version of your code. Testing against the _local_ version might work while the installed version does not (due to a missing file, changed paths, etc). This is one of the big reasons to use `/src/package` instead of `/package`, as `python -m pytest` will pick up local directories and `pytest` does not. Also, there may come a time when someone (possibly you) needs to run your tests off of a wheel or a conda package, or in a build system, and if you are unable to test against an installed version, you won't be able to run your tests! (It happens more than you might think)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock expensive or tricky calls\n",
    "\n",
    "If you have to call something that is expensive or hard to call, it is often better to mock it. To isolate parts of your own code for \"unit\" testing, mocking is useful too. Combined with monkey patching (shown in an earlier example), this is a very powerful tool!\n",
    "\n",
    "Say we want to write a function that calls matplotlib. We could use `pytest-mpl` to capture images and compare them in our test, but that's an integration test, not a unit test; and if something does go wrong, we are stuck comparing pictures, and we don't know how our usage of matplotlib changed from the test report. Let's see how we could mock it. We will use the `pytest-mock` plugin for pytest, which simply adapts the built-in `unittest.mock` in a more native pytest fashion as fixtures and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_and_run pytest --no-header --disable-pytest-warnings\n",
    "import pytest\n",
    "from pytest import approx\n",
    "import matplotlib.pyplot\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def my_plot(ax):\n",
    "    ax.plot([1,3,2], label=None, linewidth=1.5)\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_matplotlib(mocker):\n",
    "    fig = mocker.Mock(spec=matplotlib.pyplot.Figure)\n",
    "    ax = mocker.Mock(spec=matplotlib.pyplot.Axes)\n",
    "    line2d = mocker.Mock(name=\"step\", spec=matplotlib.lines.Line2D)\n",
    "    ax.plot.return_value = (line2d,)\n",
    "\n",
    "    # Patch the main library if used directly\n",
    "    mpl = mocker.patch(\"matplotlib.pyplot\", autospec=True)\n",
    "    mocker.patch(\"matplotlib.pyplot.subplots\", return_value=(fig, ax))\n",
    "\n",
    "    return SimpleNamespace(fig=fig, ax=ax, mpl=mpl)\n",
    "\n",
    "\n",
    "def test_my_plot(mock_matplotlib):\n",
    "    ax = mock_matplotlib.ax\n",
    "    my_plot(ax=ax)\n",
    "\n",
    "    assert len(ax.mock_calls) == 1\n",
    "\n",
    "    ax.plot.assert_called_once_with(\n",
    "        approx([1.0, 3.0, 2.0]),\n",
    "        label=None,\n",
    "        linewidth=approx(1.5),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just mocked the parts we touch in our plot function that we need to test. We use `spec=` to get the mock to just respond to the same things that the original object would have responded to. We can set return values so that our objects behave like the real thing. \n",
    "\n",
    "If this changes, we immediately know exactly what changed - and this runs\n",
    "instantly, we aren't making any images! While this is a little work to set up,\n",
    "it pays off in the long run.\n",
    "\n",
    "The documentation at [pytest-mock](https://pypi.org/project/pytest-mock/) is\n",
    "helpful, though most of it just redirects to the standard library\n",
    "[unittest.mock](https://docs.python.org/3/library/unittest.mock.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
